% Encoding: windows-1252

@thesis{dominikDiez,
  title = {Reinforcement Learning als Lösungsansatz für die Automatisierung von Aufgaben der Adjacent Position in Flugsicherungs-Realzeitsimulationen},
  year = {2025},
  author = {Dominik Diez},
}

@article{sb3,
  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},
  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {268},
  pages   = {1-8},
  url     = {http://jmlr.org/papers/v22/20-1364.html}
}

@misc{bluesky-gym,
  author = {Groot, DJ and Leto, G and Vlaskin, A and Moec, A and Ellerbroek, J},
  title = {BlueSky-Gym: Reinforcement Learning Environments for Air Traffic Applications},
  year = {2024},
  journal = {SESAR Innovation Days 2024},
}

@inproceedings{bluesky,
author = {Hoekstra, Jacco and Ellerbroek, Joost},
year = {2016},
month = {06},
pages = {},
title = {BlueSky ATC Simulator Project: an Open Data and Open Source Approach}
}

@article{OriginalPPO,
  author       = {John Schulman and
                  Filip Wolski and
                  Prafulla Dhariwal and
                  Alec Radford and
                  Oleg Klimov},
  title        = {Proximal Policy Optimization Algorithms},
  journal      = {CoRR},
  volume       = {abs/1707.06347},
  year         = {2017},
  url          = {http://arxiv.org/abs/1707.06347},
  eprinttype    = {arXiv},
  eprint       = {1707.06347},
  timestamp    = {Mon, 13 Aug 2018 16:47:34 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/SchulmanWDRK17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{RLProblems,
    title={Deep Reinforcement Learning Doesn't Work Yet},
    author={Irpan, Alex},
    howpublished={\url{https://www.alexirpan.com/2018/02/14/rl-hard.html}},
    year={2018}
}
@inproceedings{Optuna,
  title={{O}ptuna: A Next-Generation Hyperparameter Optimization Framework},
  author={Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  booktitle={The 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={2623--2631},
  year={2019}
}

@misc{imitationLearning,
  author = {Gleave, Adam and Taufeeque, Mohammad and Rocamonde, Juan and Jenner, Erik and Wang, Steven H. and Toyer, Sam and Ernestus, Maximilian and Belrose, Nora and Emmons, Scott and Russell, Stuart},
  title = {imitation: Clean Imitation Learning Implementations},
  year = {2022},
  howPublished = {arXiv:2211.11972v1 [cs.LG]},
  archivePrefix = {arXiv},
  eprint = {2211.11972},
  primaryClass = {cs.LG},
  url = {https://arxiv.org/abs/2211.11972},
}

@misc{sutton2018rl,
  title={RL: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@misc{OpenAI-Gym,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  Eprint = {arXiv:1606.01540},
}

@article{gymnasium,
  title={Gymnasium: A Standard Interface for Reinforcement Learning Environments},
  author={Towers, Mark and Kwiatkowski, Ariel and Terry, Jordan and Balis, John U and De Cola, Gianluca and Deleu, Tristan and Goul{\~a}o, Manuel and Kallinteris, Andreas and Krimmel, Markus and KG, Arjun and others},
  journal={arXiv preprint arXiv:2407.17032},
  year={2024}
}

@inproceedings{CPA,
  title={Methodology for estimation of closest point of approach between aircraft in ATM},
  author={Errico, Angela and Di Vito, Vittorio},
  booktitle={AIAA Aviation 2019 Forum},
  pages={2832},
  year={2019}
}

@Online{sb3_plus,
  author   = {adysonmaia},
  title    = {SB3-Plus},
  url      = {https://github.com/adysonmaia/sb3-plus},
  accessed = {2026-01-12},
}

@Online{sb3_contrib,
  author   = {Stable-Baselines3 Contributors},
  title    = {Stable-Baselines3 Contrib (sb3-contrib)},
  url      = {https://github.com/Stable-Baselines-Team/stable-baselines3-contrib},
  accessed = {2026-02-12},
}

@Online{sb3_algos_doc,
  author   = {Stable-Baselines3 Contributors},
  title    = {Stable-Baselines3 Documentation: RL Algorithms},
  url      = {https://stable-baselines3.readthedocs.io/en/master/guide/algos.html},
  accessed = {2026-02-12},
}

@Online{torch_detach,
  author   = {PyTorch},
  title    = {torch.Tensor.detach},
  url      = {https://docs.pytorch.org/docs/stable/generated/torch.Tensor.detach.html},
  accessed = {2026-01-12},
}

@article{invalid_Action_Masking,
   title={A Closer Look at Invalid Action Masking in Policy Gradient Algorithms},
   volume={35},
   ISSN={2334-0762},
   url={http://dx.doi.org/10.32473/flairs.v35i.130584},
   DOI={10.32473/flairs.v35i.130584},
   journal={The International FLAIRS Conference Proceedings},
   publisher={University of Florida George A Smathers Libraries},
   author={Huang, Shengyi and Ontañón, Santiago},
   year={2022},
   month=may }

@misc{statistical_rl_evaluation,
   title={Deep Reinforcement Learning at the Edge of the Statistical Precipice}, 
   author={Rishabh Agarwal and Max Schwarzer and Pablo Samuel Castro and Aaron Courville and Marc G. Bellemare},
   year={2022},
   eprint={2108.13264},
   archivePrefix={arXiv},
   primaryClass={cs.LG},
   url={https://arxiv.org/abs/2108.13264}, 
}

@Online{lunar_lander_doc,
  author   = {Oleg Klimov},
  title    = {LunarLander Environment Documentation},
  url      = {https://gymnasium.farama.org/environments/box2d/lunar_lander/#arguments},
  accessed = {2026-01-23},
}

@Online{algo_score_reference,
  author   = {Deutsch Zentrum für Luft- und Raumfahrt},
  title    = {RL Baselines3 Zoo - Performance of trained agents},
  url      = {https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/benchmark.md},
  accessed = {2026-01-28},
}

@misc{branching_architectures,
      title={Action Branching Architectures for Deep Reinforcement Learning}, 
      author={Arash Tavakoli and Fabio Pardo and Petar Kormushev},
      year={2019},
      eprint={1711.08946},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1711.08946}, 
}

@Inbook{PyTorch_Fundamentals,
author="Chaubal, Siddhesh Prashant",
title="Tensors in PyTorch",
bookTitle="AI Projects in PyTorch: Hands-On Projects in Vision, Text, and Generative Models",
year="2025",
publisher="Apress",
address="Berkeley, CA",
pages="23--49",
abstract="This chapter introduces the reader to tensors, the foundational data structure of PyTorch. We start the chapter by introducing tensors in PyTorch, building from the simplest 1D tensors to 2D and then 3D tensors. We gradually build intuition by fixing one dimension at a time and then introduce basic operations like indexing and slicing. We give a glimpse of how tensors are stored in memory. Then we look at different ways of initializing tensors and describe the attributes of tensors. We give a bird's-eye view of tensor operations belonging to different categories to make the reader aware of all that PyTorch has to offer in terms of tensor operations. We then describe how PyTorch's autograd mechanism computes gradients by constructing computational graphs and also go over the high-level recipe that we typically use for model training by leveraging PyTorch's autograd mechanism. We end the chapter with a series of exercises designed to improve your understanding of PyTorch tensors.",
isbn="979-8-8688-2117-2",
doi="10.1007/979-8-8688-2117-2_2",
url="https://doi.org/10.1007/979-8-8688-2117-2_2"
}

@article{CHEN2023104367,
title = {General real-time three-dimensional multi-aircraft conflict resolution method using multi-agent reinforcement learning},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {157},
pages = {104367},
year = {2023},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2023.104367},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X23003571},
author = {Yutong Chen and Yan Xu and Lei Yang and Minghua Hu},
keywords = {Air traffic management, Three-dimensional multi-aircraft conflict resolution, Multi-agent reinforcement learning, Deep q-learning network, Generalisation, Uncertainty},
abstract = {Reinforcement learning (RL) techniques have been studied for solving the conflict resolution (CR) problem in air traffic management, leveraging their potential for computation and ability to handle uncertainty. However, challenges remain that impede the application of RL methods to CR in practice, including three-dimensional manoeuvres, generalisation, trajectory recovery, and success rate. This paper proposes a general multi-agent reinforcement learning approach for real-time three-dimensional multi-aircraft conflict resolution, in which agents share a neural network and are deployed on each aircraft to form a distributed decision-making system. To address the challenges, several technologies are introduced, including a partial observation model based on imminent threats for generalisation, a safety separation relaxation model for multiple flight levels for three-dimensional manoeuvres, an adaptive manoeuvre strategy for trajectory recovery, and a conflict buffer model for success rate. The Rainbow Deep Q-learning Network (DQN) is used to enhance the efficiency of the RL process. A simulation environment that considers flight uncertainty (resulting from mechanical and navigation errors and wind) is constructed to train and evaluate the proposed approach. The experimental results demonstrate that the proposed method can resolve conflicts in scenarios with much higher traffic density than in today’s real-world situations.}
}

@article{lillicrap2015continuous,
  title={CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}
@Online{separation_standards,
  author   = {skybrary},
  title    = {Separation Standards},
  url      = {https://skybrary.aero/articles/separation-standards},
  accessed = {2026-02-05},
}

@inproceedings{masson2016reinforcement,
  title={Reinforcement learning with parameterized actions},
  author={Masson, Warwick and Ranchod, Pravesh and Konidaris, George},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={30},
  number={1},
  year={2016}
}

@book{bedinteEndropie,
  title={Elements of information theory},
  author={Cover, Thomas M},
  year={1999},
  publisher={John Wiley \& Sons}
}

@Online{gradientenabstieg_visio,
  author   = {Abhay singh},
  title    = {Gradient Descent Explained: The Engine Behind AI Training},
  url      = {https://medium.com/@abhaysingh71711/gradient-descent-explained-the-engine-behind-ai-training-2d8ef6ecad6f},
  accessed = {2026-02-10},
}

@Online{gymnasium_basic_usage,  
  author   = {Farama Foundation},
  title    = {Gymnasium: Basic Usage},
  url      = {https://gymnasium.farama.org/introduction/basic_usage/},
  accessed = {2026-02-12},
}

@Online{LogExplanation,  
  author   = {Deutsches Zentrum für Luft- und Raumfahrt},
  title    = {Stable Baselines3: Explanation of Logger Output},
  url      = {https://stable-baselines3.readthedocs.io/en/master/common/logger.html#explanation-of-logger-output},
  accessed = {2026-02-12},
}

@inproceedings{bamford2021generalising,
  title={Generalising discrete action spaces with conditional action trees},
  author={Bamford, Christopher and Ovalle, Alvaro},
  booktitle={2021 IEEE Conference on Games (CoG)},
  pages={1--8},
  year={2021},
  organization={IEEE}
}

@inproceedings{haarnoja2018soft,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1861--1870},
  year={2018},
  organization={Pmlr}
}

