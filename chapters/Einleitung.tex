\chapter{Einleitung}
\label{ch:intro}
Diese Arbeit wurde im Rahmen des Dualen Studiums der Informatik an der Hochschule Darmstadt in Kooperation mit der \ac{DFS} erstellt.
Die Inhalte dieser Arbeit basieren zum Teil auf den Anforderungen, Erfahrungen und Erkenntnissen, die im Rahmen des Praxisprojekts der dritten Praxisphase im Team 
TI/TD der \ac{DFS} gesammelt wurden. Dieses Projekt baut auf einer vorherigen Bachelorarbeit \cite{dominikDiez} auf, die Reinforcement Learning als Lösungsansatz für die teilautomatisierte Flugverkehrssteuerung in der Adjacent-Position untersucht hat.

%
% Section: Motivation
%
\section{Motivation}
\label{sec:intro:motivation}
Das Team TI/TD der \ac{DFS} entwickelt Simulatoren, die in der Aus- und Weiterbildung von Fluglotsen eingesetzt werden. 
Während der Simulatorübungen arbeiten die Trainees mit den Adjacent-Lotsen zusammen.
\vspace{\baselineskip}

TI/TD betreut mehrere Simulatoren, darunter auch NewSim Web, eine Ausprägung des NewSim En-Route Simulators, welcher im Browser verwendet werden kann.
Dieser soll es Lotsen ermöglichen, von Zuhause selbstständig zu üben. Speziell in diesem Szenario wäre ein KI-Adjacent-Lotse eine denkbare und sinnvolle Erweiterung,
die den Trainees zusätzlich zu den im Vorhinein speziell gebauten Szenarien zur Verfügung stehen könnte.
%
% Section: Ziele
%
\section{Ziel der Arbeit}
\label{sec:intro:goal}

Ziel dieser Arbeit ist die Konzeption und Evaluation eines \ac{RL}-Systems, das die konfliktfreie Steuerung des Flugverkehrs in den umliegenden Sektoren der Trainees übernimmt. 
Das System soll Steueranweisungen (Pilot-Commands) generieren, um Flugkonflikte zu vermeiden und den Verkehr effizient zu steuern. 
Dabei werden die Realisierbarkeit der Anweisungen, physikalische Limitierungen und die Anweisungsfrequenz berücksichtigt. 
Die Arbeit soll folgende Fragestellung beantworten:
\begin{itemize}
    \item Wie lassen sich ein RL-Agent und dessen Trainingsumgebung gestalten, um einerseits konfliktarmes, andererseits aber auch effizientes Lotsen durch den Agenten zu erreichen?
\end{itemize}
Daraus leiten sich folgende Unterfragen ab:
\begin{itemize}
    \item Wie sollte der Aktionsraum gestaltet werden, um ein bestmögliches Ergebnis zu erzielen?
    \item Lässt sich ein RL-Agent mit diskreten und kontinuierlichen Aktionen so konzipieren, dass Noop-Entscheidungen korrekt abgebildet werden und kontinuierliche Aktionen nur bei Bedarf trainiert werden?
    \item Wie kann ein aktionsabhängiges Gradient-Gating implementiert werden, um das Training durch Aktionsmaskierung nicht negativ zu beeinträchtigen?
    \item Überwiegen die Vorteile eines RL-Agenten mit diskreten und kontinuierlichen Aktionen gegenüber einem klassischen (rein diskreten) Agenten?
\end{itemize}


Diese Arbeit verfolgt nicht das Ziel, ein operativ einsetzbares System zur vollständigen Ablösung des Adjacent-Lotsen zu entwickeln. 
Vielmehr wird ein Proof of Concept vorgestellt, der auf einem stark vereinfachten Szenario basiert und die grundlegende Machbarkeit sowie zentrale Herausforderungen der Anwendung von \ac{RL} im Flugverkehrsmanagement untersucht.
% Section: Struktur der Arbeit
%
\section{Gliederung}
\label{sec:intro:structure}
Zu Beginn der Arbeit werden in Kapitel \ref{ch:fs_grundlagen} die Grundlagen der Problemdomäne Flugverkehrssteuerung und die Rolle der Adjacent-Lotsen in der Ausbildung von Fluglotsen am Simulator erläutert.
Anschließend werden in Kapitel \ref{ch:rl_grundlagen} die theoretischen Grundlagen der Lösungsdomäne (\ac{RL}) und relevanten Algorithmen vorgestellt. 
Danach wird in Kapitel \ref{ch:Statistische_Auswertung} auf die statistischen Methoden zur Auswertung der Experimente eingegangen.
In Kapitel \ref{ch:konzeption} werden die Definition von Zustands- und Aktionsräumen sowie der Belohnungsfunktion behandelt, sowie deren Zusammenhang mit verschiedenen Konfliktlösungsmaßnahmen erläutert.
Kapitel \ref{ch:evaluation} widmet sich der Evaluation des \ac{RL}-Systems, in welchem die durchgeführten Experimente und deren Ergebnisse präsentiert und analysiert werden.
Abschließend werden in Kapitel \ref{ch:zusammenfassung} die Ergebnisse der Arbeit zusammengefasst und ein Ausblick auf mögliche zukünftige Arbeiten gegeben.

% Auswertung in genauer beschreibem, Statistische segnifikanz, Wiederholbarkeit usw. Porblm aktuelle dass das projekt weniger Architektur usw ist soder n ehrr parametrisierung. Literaturrschersche. 
%Genaueres Exposee in einem getrennten Dokument.Raschersche wie de performance von RL systemen in anderen arbeiten bwertet wird
https://medium.com/@digitalconsumer777/evaluating-reinforcement-learning-algorithms-metrics-and-benchmarks-e796d03cb81c