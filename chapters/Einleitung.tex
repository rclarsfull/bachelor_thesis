\chapter{Einleitung}
\label{ch:intro}
Diese Arbeit wurde im Rahmen des Dualen Studiums der Informatik an der Hochschule Darmstadt in Kooperation mit der \ac{DFS} erstellt.
Die Inhalte dieser Arbeit basieren zum Teil auf den Anforderungen, Erfahrungen und Erkenntnissen, die im Rahmen des Praxisprojekts der dritten Praxisphase im Team 
TI/TD der \ac{DFS} gesammelt wurden. Dieses Projekt baut auf einer vorherigen Bachelorarbeit \cite{dominikDiez} auf, die Reinforcement Learning als Lösungsansatz für die teilautomatisierte Flugverkehrssteuerung in der Adjacent-Position untersucht hat.

%
% Section: Motivation
%
\section{Motivation}
\label{sec:intro:motivation}
Das Team TI/TD der \ac{DFS} entwickelt Simulatoren, die in der Aus- und Weiterbildung von Fluglotsen eingesetzt werden. 
Während der Simulatorübungen arbeiten die Trainees mit den Adjacent-Lotsen zusammen.
\vspace{\baselineskip}

TI/TD betreut mehrere Simulatoren, darunter auch NewSim Web, eine Ausprägung des NewSim En-Route Simulators, welcher im Browser verwendet werden kann.
Dieser soll es Lotsen ermöglichen, von Zuhause selbstständig zu üben. Speziell in diesem Szenario wäre ein KI-Adjacent-Lotse eine denkbare und sinnvolle Erweiterung,
die den Trainees zusätzlich zu den im Vorhinein speziell gebauten Szenarien zur Verfügung stehen könnte.
%
% Section: Ziele
%
\section{Ziel der Arbeit}
\label{sec:intro:goal}

Ziel dieser Arbeit ist die Entwicklung eines \ac{RL}-Agenten für die Adjacent-Position, der nicht nur Konfliktfreiheit gewährleistet, sondern auch ein effizientes und vor allem ruhiges Steuerungsverhalten zeigt. Im Gegensatz zu klassischen Ansätzen soll das häufige "Jittering" – also unnötige, kleinteilige Kurskorrekturen – durch die Einführung eines gemischten Aktionsraums mit expliziter \emph{No-Operation}-Option minimiert werden.

Die zentrale Forschungsfrage lautet daher:
\begin{itemize}
    \item Wie muss die Architektur eines Reinforcement-Learning-Agenten sowie dessen Trainingsumgebung beschaffen sein, um im Kontext der Flugverkehrssteuerung sowohl Konfliktfreiheit als auch eine hohe Effizienz der Lotsentätigkeit zu gewährleisten?
\end{itemize}

Zur Beantwortung dieser Frage werden vier konkrete Untersuchungsziele definiert, die für die spätere Evaluation maßgeblich sind:
\begin{itemize}
    \item \textbf{Abbildung von Noops:} In welcher Form lässt sich eine explizite Repräsentation von Nicht-Eingriffen (\emph{No-Operation}) im Aktionsraum realisieren, die zur Vermeidung redundanter Steuerbefehle führt?
    \item \textbf{Architektonische Machbarkeit:} Ermöglicht eine Architektur mit hybridem Aktionsraum (diskret und kontinuierlich) dem Agenten eine effektive Differenzierung zwischen aktiven Steuerbefehlen und dem Verzicht auf Eingriffe?
    \item \textbf{Algorithmische Machbarkeit:} Lässt sich der Proximal-Policy-Optimization-Algorithmus (PPO) durch die Integration von \emph{Gradient Gating} derart modifizieren, dass ein stabiles Lernverhalten trotz der bedingten Abhängigkeiten im gemischten Aktionsraum ohne Beeinträchtigung der Konvergenz gewährleistet ist?
    \item \textbf{Operativer Mehrwert:} Resultiert aus dem erhöhten architektonischen Aufwand ein quantifizierbarer operativer Mehrwert – insbesondere im Hinblick auf eine reduzierte Eingriffsfrequenz, effizientere Flugführung oder gesteigerte Erfolgsrate – im Vergleich zu einem Agenten mit rein diskretem Aktionsraum?
\end{itemize}


Diese Arbeit verfolgt nicht das Ziel, ein operativ einsetzbares System zur vollständigen Ablösung des Adjacent-Lotsen zu entwickeln. 
Vielmehr wird ein Proof of Concept vorgestellt, der auf einem stark vereinfachten Szenario basiert und die grundlegende Machbarkeit sowie zentrale Herausforderungen der Anwendung von \ac{RL} im Flugverkehrsmanagement untersucht.
% Section: Struktur der Arbeit
%
\section{Gliederung}
\label{sec:intro:structure}
Zu Beginn der Arbeit werden in Kapitel \ref{ch:fs_grundlagen} die Grundlagen der Problemdomäne Flugverkehrssteuerung und die Rolle der Adjacent-Lotsen in der Ausbildung von Fluglotsen am Simulator erläutert.
Anschließend werden in Kapitel \ref{ch:rl_grundlagen} die theoretischen Grundlagen der Lösungsdomäne (\ac{RL}) und relevanten Algorithmen vorgestellt. 
Danach wird in Kapitel \ref{ch:Statistische_Auswertung} auf die statistischen Methoden zur Auswertung der Experimente eingegangen.
In Kapitel \ref{ch:konzeption} werden die Definition von Zustands- und Aktionsräumen sowie der Belohnungsfunktion behandelt, sowie deren Zusammenhang mit verschiedenen Konfliktlösungsmaßnahmen erläutert.
Darauf aufbauend beschreibt Kapitel \ref{ch:ppo_bedingt} die notwendigen algorithmischen Anpassungen des PPO\hyp{}Algorithmus, um die Besonderheiten des zuvor definierten gemischten Aktionsraums effizient zu handhaben.
Kapitel \ref{ch:evaluation} widmet sich der Evaluation des \ac{RL}-Systems, in welchem die durchgeführten Experimente und deren Ergebnisse präsentiert und analysiert werden.
Abschließend werden in Kapitel \ref{ch:zusammenfassung} die Ergebnisse der Arbeit zusammengefasst und ein Ausblick auf mögliche zukünftige Arbeiten gegeben.

