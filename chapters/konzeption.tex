\chapter{Umgebungs-Konzeption}
\label{ch:konzeption}

Nachdem die theoretischen Fundamente der Flugsicherung und des Reinforcement Learning gelegt sind, widmet sich dieses Kapitel der konkreten Überführung der Problemstellung in ein trainierbares System. Die Konzeption der Lernumgebung ist der entscheidende Entwicklungsschritt, in dem abstrakte Anforderungen in ein mathematisch greifbares \ac{MDP} übersetzt werden.
\vspace{\baselineskip}

Die Qualität der hier getroffenen Designentscheidungen – von der Auswahl der Zustandsmerkmale bis zur Ausgestaltung der Belohnungsfunktion – bestimmt maßgeblich, ob der Agent in der Lage sein wird, nicht nur konfliktfreie, sondern auch operativ sinnvolle und effiziente Strategien zu entwickeln. Ein besonderer Schwerpunkt liegt dabei auf der Modellierung des Aktionsraums, da hier der zentrale Hebel zur Reduktion unnötiger Steuerbefehle (Jittering) vermutet wird. Zu diesem Zweck werden zwei konkurrierende Ansätze vorgestellt: ein klassischer diskreter Aktionsraum und ein neuartiger gemischter Ansatz.
\vspace{\baselineskip}

Im Folgenden wird zunächst die technische Integration des Flugsimulators Bluesky sowie die Generierung relevanter Trainingsszenarien erläutert. Darauf aufbauend erfolgt die detaillierte Definition der MDP-Komponenten, bevor abschließend die Wahl des PPO-Algorithmus als geeignetes Lernverfahren begründet wird.

\section{Anforderungen aus der Problemdomäne}
Vor der detaillierten Beschreibung des \ac{RL}\hyp{}Systems ist ein Verständnis der spezifischen Anforderungen der Flugverkehrskontrolle essenziell. 
Der Lotse im angrenzenden Sektor (Adjacent) muss Flugkonflikte frühzeitig erkennen und lösen, wobei gleichzeitig die Effizienz des Verkehrsflusses gewahrt bleiben muss.
Effizienz impliziert in diesem Kontext, dass Flugzeuge möglichst auf direkten Routen geführt werden und unnötige Kurskorrekturen vermieden werden.
Jede Kursänderung erfordert in der Praxis eine Funkkommunikation zwischen Lotse und Pilot, was die kognitive Arbeitsbelastung (Workload) erhöht und die Frequenzbelegung steigert.
Der \ac{RL}\hyp{}Agent soll daher lernen, Konflikte unter Minimierung von Steuerbefehlen zu lösen, um ein realistisches Lotsenverhalten zu emulieren.

\section{Verwandte Arbeiten und Abgrenzung}
Die Anwendung von Reinforcement Learning im Kontext der Flugverkehrskontrolle ist ein aktives Forschungsfeld, das sich mit der Automatisierung von Entscheidungsprozessen in komplexen, dynamischen Umgebungen beschäftigt.
Viele Arbeiten konzentrieren sich auf die Entwicklung von Agenten, die in der Lage sind, Konflikte zu erkennen und zu lösen, wobei jedoch die Effizienz der Steuerbefehle oft nicht explizit adressiert wird.

Die Arbeit von Groot et al. \cite{bluesky-gym} hat diese Arbeit maßgeblich beeinflusst, da sie eine open source Implementierung einer Bluesky\hyp{}Gym\hyp{}Umgebung bereitgestellt haben, die als Grundlage für die Entwicklung des hier vorgestellten \ac{RL}\hyp{}Systems diente.
Diese Arbeit erweitert die bisherigen Ansätze, indem sie nicht nur die Konfliktlösung, sondern auch die Minimierung von Steuerbefehlen in den Fokus stellt. 
Die Einführung eines gemischten Aktionsraums mit expliziter \emph{No-Operation}-Option stellt eine innovative Erweiterung dar, die es dem Agenten ermöglicht, zwischen aktiven Steuerbefehlen und dem Verzicht auf Eingriffe zu differenzieren, was in der Literatur bisher wenig Beachtung gefunden hat.

\section{Szenarien}
Die Szenarien wurden so gestaltet, dass der Agent mit einer Vielzahl an Verkehrssituationen konfrontiert wird, was essenziell für die Generalisierungsfähigkeit des Modells ist. 
Die Generierung der Szenarien erfolgt stochastisch, folgt jedoch definierten Mustern, um eine breite Abdeckung möglicher Konfliktgeometrien zu gewährleisten. 
Es werden vier grundlegende Szenarientypen im Training unterschieden:
\begin{itemize}
    \item \textbf{Crossing:} Erzeugung von Konflikten durch sich kreuzende Flugrouten.
    \item \textbf{Merging:} Simulation von zusammenlaufendem Verkehr, bei dem Flugzeuge von individuellen Startpunkten auf einen gemeinsamen Zielpunkt oder Wegpunkt zufliegen.
    \item \textbf{Diverging:} Simulation von auseinanderlaufendem Verkehr. Flugzeuge starten gestaffelt auf einer identischen Route, die sich im weiteren Verlauf aufzweigt.
    \item \textbf{Scenariofiles:} Verwendung vordefinierter Szenarien aus externen Dateien, die auf realen Übungen der Fluglotsenausbildung basieren. 
    Diese Szenarien wurden für die Simulation abstrahiert, da das aktuelle Modell ausschließlich laterale Steuerung (Kursänderungen) betrachtet und Höhen\hyp{} oder Geschwindigkeitsanpassungen ausgeklammert werden.
\end{itemize}

Die Auswahl des Szenariotyps während des Trainings erfolgt randomisiert, wobei Crossing\hyp{} und Merging\hyp{}Szenarien aufgrund ihrer höheren Komplexität bei der Konfliktlösung häufiger priorisiert werden.

\section{Simulatorintegration und Environment}
Als Simulationskern dient Bluesky \cite{bluesky}, eine Open\hyp{}Source\hyp{}Flugsimulationsplattform, die eine realistische Modellierung der Flugzeugdynamik bietet und Schnittstellen für externe Steuerungssysteme bereitstellt.
Die Interaktion zwischen der Reinforcement\hyp{}Learning\hyp{}Umgebung (Gym) und Bluesky erfolgt über eine dedizierte Adapter\hyp{}Schicht. Diese übersetzt die Aktionen des Agenten in simulatorverständliche Steuerbefehle und extrahiert im Gegenzug die relevanten Zustandsinformationen für das Training.
\vspace{\baselineskip}

Die modulare Architektur der Adapter\hyp{}Schicht erlaubt einen potenziellen Austausch des Simulators, was im Hinblick auf eine geplante Migration auf den Simulator NewSim der DFS von Bedeutung ist.
Die Entscheidung für Bluesky fiel primär aufgrund der Implementierung in Python. 
Dies ermöglichte eine direkte Integration in das Python\hyp{}basierte \ac{RL}\hyp{}Framework ohne die Notwendigkeit komplexer Schnittstellen für den Datenaustausch zwischen unterschiedlichen Programmiersprachen, was die Implementierungskomplexität signifikant verringerte.
Zudem konnte auf Vorarbeiten des Projekts Bluesky\hyp{}Gym \cite{bluesky-gym} aufgebaut werden.

\section{Technische Optimierung der Lernumgebung}
Ein kritischer, oft unterschätzter Faktor im Reinforcement Learning ist die Ausführungsgeschwindigkeit der Umgebung. Da moderne RL-Algorithmen wie PPO typischerweise Millionen von Interaktionsschritten benötigen, um zu konvergieren, summieren sich selbst minimale Verzögerungen pro Zeitschritt zu erheblichen Wartezeiten. Eine performante Implementierung ist daher nicht nur eine Frage der Effizienz, sondern bestimmt maßgeblich die Entwicklungsgeschwindigkeit, da sie schnellere Iterationszyklen und damit häufigeres Testen von Hypothesen ermöglicht.
\vspace{\baselineskip}

Um diesem Anspruch gerecht zu werden, wurde der Python-Code der Umgebung gezielt auf Performance optimiert. Ein Nadelöhr in Python-basierten Simulationen ist oft der Overhead des Interpreters bei rechenintensiven Schleifen. Zur Beschleunigung kam die Just-in-Time Compiler-Bibliothek \emph{Numba} zum Einsatz. Numba übersetzt Python-Funktionen zur Laufzeit in optimierten Maschinencode, was insbesondere bei mathematischen Berechnungen – wie der Distanzbestimmung zwischen Flugzeugen oder der Koordinatentransformation für den relativen Zustandsraum – zu Geschwindigkeitssteigerungen in der Größenordnung von kompilierten Sprachen führt.
\vspace{\baselineskip}

Zusätzlich wurden Berechnungen wo immer möglich vektorisiert. Statt über Listen von Flugzeugen zu iterieren, werden Operationen mittels der Bibliothek \emph{NumPy} auf ganzen Arrays gleichzeitig ausgeführt. Dies nutzt moderne CPU-Instruktionen (SIMD) effizient aus und minimiert teure Funktionsaufrufe und Kontextwechsel innerhalb der Python-Laufzeitumgebung. Durch diese Kombination aus Kompilierung und Vektorisierung konnte der Durchsatz der Umgebung (Steps per Second) massiv gesteigert werden, was die Experimentierzyklen von Tagen auf Stunden reduzierte.

\section{Visualisierung}
Zur Validierung und Analyse des Agentenverhaltens wurde eine Echtzeit\hyp{}Visualisierung implementiert.
Diese stellt neben der Verkehrslage auch erweiterte Diagnoseinformationen dar, darunter prognostizierte Konfliktpunkte, die aktuellen Entscheidungen des Agenten sowie relevante Merkmale des Zustandsraums.
Die technische Umsetzung erfolgte mittels der Bibliothek Pygame, welche eine effiziente und plattformunabhängige Erstellung von 2D\hyp{}Grafiken innerhalb des Python\hyp{}Ökosystems ermöglicht.

\section{Definition der RL\hyp{}Umgebung}
Die Umgebung (Environment) bildet die Schnittstelle zwischen dem \ac{RL}\hyp{}Agenten und der Simulation. 
Sie definiert den Zustandsraum, den Aktionsraum sowie die Belohnungsfunktion.
Die Implementierung folgt der Spezifikation von OpenAI Gym \cite{OpenAI-Gym}, um die Kompatibilität mit etablierten \ac{RL}\hyp{}Bibliotheken wie Stable Baselines3 \cite{sb3} sicherzustellen.

\subsection{Zustandsraum}
Das Design des Zustandsraums zielt darauf ab, dem Agenten alle relevanten Informationen zur Konflikterkennung und \hyp{}lösung bereitzustellen, ohne die Generalisierungsfähigkeit zu beeinträchtigen. 
Eine hohe Generalisierung ist erforderlich, damit der Agent auch in unbekannten Situationen, die nicht Teil der Trainingsdaten waren, robuste Entscheidungen trifft.
\vspace{\baselineskip}

Absolute Positionsdaten erwiesen sich als ungeeignet, da sie stark an spezifische Szenarien gebunden sind. 
Stattdessen werden relative Positionsdaten verwendet, die eine abstraktere und damit besser generalisierbare Repräsentation der Verkehrslage ermöglichen.
\vspace{\baselineskip}

Ein weiterer technischer Aspekt ist die stetige Repräsentation zyklischer Merkmale. 
Die direkte Verwendung von Winkelgraden ist problematisch, da die Werte $0^\circ$ und $360^\circ$ numerisch weit auseinanderliegen, obwohl sie identische Zustände beschreiben. 
Um Sprungstellen im Zustandsraum zu vermeiden, werden Winkel durch ihre Sinus\hyp{} und Kosinuskomponenten kodiert.
\vspace{\baselineskip}

Da neuronale Netze in der Regel Eingabevektoren fester Länge erwarten, muss die Dimension des Zustandsraums unabhängig von der variablen Anzahl der Flugzeuge im Sektor konstant bleiben. 
Dies wird durch die Festlegung einer maximalen Slot\hyp{}Anzahl für beobachtbare Flugzeuge und das Auffüllen (Padding) ungenutzter Slots mit neutralen Werten erreicht. 
Die Auswahl, welche Flugzeuge in den Zustandsvektor aufgenommen werden, erfolgt priorisiert nach deren Konfliktpotential.
Dieses wird mittels der \ac{CPA}\hyp{}Methode \cite{CPA} bestimmt, wobei die Zeit bis zum nächsten Konflikt sowie die Anzahl der prognostizierten Konflikte die ausschlaggebenden Metriken darstellen.
\vspace{\baselineskip}

Der Zustandsraum beider Umgebungen umfasst folgende Merkmale:
\begin{itemize}
    \item \textbf{Eigener Agent:}
    \begin{itemize}
        \item Aktuelles Heading (Sinus/Kosinus)
        \item Aktuelle Geschwindigkeit
        \item Kursabweichung (Heading\hyp{}Deviation) zum nächsten Wegpunkt
        \item Distanz zum nächsten Wegpunkt
        \item Letzte ausgeführte Aktion (exklusive Noop)
        \item Zeitdifferenz seit der letzten nicht Noop Aktion
        \item Aktuelle Drehrate (Rate of Turn)
        \item Prognostizierte Zeit bis zum Konflikt für verschiedene Kurs-Offsets (z.\,B. $-15^\circ, 0^\circ, +15^\circ$)
        \item Prognostizierte Zeit bis zum Konflikt bei direktem Kurs zum Ziel
    \end{itemize}
    \item \textbf{Konfliktverkehr (sortiert nach Kritikalität):}
    \begin{itemize}
        \item Relativer Positionsvektor
        \item Euklidische Distanz
        \item Relativer Winkel (Sinus/Kosinus)
        \item Geschwindigkeit des Flugzeugs
        \item Annäherungsrate (Closing Rate)
        \item Minimaler Abstand bei konstantem Kurs (\ac{CPA}\hyp{}Distanz)
        \item Zeit bis zum minimalen Abstand (Time to \ac{CPA})
    \end{itemize}
    \item \textbf{Umgebungsverkehr (die 4 nächsten Flugzeuge):}
    \begin{itemize}
        \item Heading (Sinus/Kosinus)
        \item Geschwindigkeit
        \item Kursabweichung zum nächsten Wegpunkt
        \item Distanz zum nächsten Wegpunkt
    \end{itemize}
\end{itemize}

Die Berechnung zukünftiger Konflikte basiert auf dem \ac{CPA}\hyp{}Verfahren (Closest Point of Approach) \cite{CPA}, wobei die Zeit bis zum Konflikt (\emph{Time to CPA}) als Maß für die Kritikalität dient.
\subsection{Aktionsraum}
\label{sc:aktionsraum}
Die entwickelte Lernumgebung beschränkt sich auf die Konfliktlösung mittels Kursänderungen (Lateralsteuerung). 
Diese Eingrenzung dient der Reduktion der Komplexität des Modells. 
Obgleich operative Fluglotsen auch über Geschwindigkeits\hyp{} und Höhenänderungen verfügen, bleibt die Integration dieser Freiheitsgrade zukünftigen Untersuchungen vorbehalten.
\vspace{\baselineskip}

Ein rein kontinuierlicher Aktionsraum erwies sich für die Anforderungen als unzureichend, da er die Modellierung einer expliziten \emph{Noop}-Aktion (No Operation) – also den bewussten Verzicht auf einen Eingriff – erschwert. 
Ein kontinuierlicher Output von $0^\circ$ ist funktional nicht äquivalent zu \emph{Noop}, insbesondere unter Berücksichtigung der Latenz bei der Befehlsumsetzung. 
Wenn im vorangegangenen Zeitschritt eine signifikante Kursänderung eingeleitet wurde, ermöglicht \emph{Noop} dem Agenten, dieses Manöver ohne weitere Störimpulse abzuschließen.
Ein zentrales Lernziel ist die Minimierung der Interventionsrate: Der Agent soll idealerweise nur zwei Aktionen pro Konflikt ausführen – das Ausweichmanöver und die anschließende Rückkehr auf die Route (Direct Route).
\vspace{\baselineskip}

Der Aktionsraum wurde in zwei Varianten implementiert:
\begin{itemize}
    \item \textbf{Diskreter Aktionsraum:} Der Agent wählt aus einem finiten Set an Aktionen, bestehend aus festen Kursänderungen (z.\,B. $-15^\circ, -5^\circ, \dots, +15^\circ$), \emph{Noop} und \emph{Direct Route}.
    Dieser Ansatz eignet sich für grobe Manöver, stößt jedoch bei feingranularer Steuerung an Grenzen. Wie in Abschnitt \ref{sc:action_extraction} ausführlich hergeleitet, führt eine Erhöhung der Auflösung zu einer drastischen Zunahme der Modellkomplexität (Ausgangsneuronen), weshalb hier eine limitierte Auswahl an diskreten Schritten getroffen wurde.
    \item \textbf{Gemischter Aktionsraum (Hybrid):} Die Entscheidung erfolgt hierarchisch über einen diskreten Aktionstyp und einen zugehörigen kontinuierlichen Parameter:
    \begin{itemize}
        \item \textbf{Steer:} Der Agent bestimmt eine Kursänderung als kontinuierlichen Wert im Intervall $[-180^\circ, +180^\circ]$.
        \item \textbf{Noop:} Der Agent behält den aktuellen Status bei.
        \item \textbf{Direct Route:} Der Agent veranlasst die Rückkehr auf den direkten Kurs zum nächsten Wegpunkt.
    \end{itemize}
\end{itemize}

Die Implementierung des gemischten Aktionsraums stellte besondere Anforderungen an den Lernalgorithmus. 
Kapitel \ref{ch:ppo_bedingt} erläutert die notwendigen Modifikationen am PPO\hyp{}Algorithmus zur Handhabung bedingter Aktionsräume.

\subsection{Belohnungsfunktion}
Das Design der Belohnungsfunktion (Reward Function) erfordert eine sorgfältige Abwägung zwischen den konkurrierenden Zielen der Konfliktvermeidung und der Flugeffizienz.
Um die Komplexität des Lernproblems handhabbar zu halten, wurde die Anzahl der Einflussfaktoren auf ein notwendiges Minimum beschränkt.
Eine übermäßig komplexe Reward\hyp{}Struktur kann die Konvergenz des Modells erschweren, da die Kausalität zwischen Aktion und Belohnung für den Agenten schwerer zu extrahieren ist.
\vspace{\baselineskip}

Die implementierte Belohnungsfunktion setzt sich aus folgenden Komponenten zusammen:
\begin{itemize}
    \item \textbf{Drift Reward:} Belohnung für das Einhalten des direkten Kurses zum Ziel. Die Funktion skaliert quadratisch mit der Kursabweichung, um signifikante Abweichungen überproportional stärker zu bestrafen als minimale, wodurch eine hohe Präzision gefördert wird. Dieser Mechanismus sorgt dafür, dass die grobe Richtung stets stimmt, der Agent jedoch nicht zu übermäßigem Mikromanagement bei kleinsten Abweichungen verleitet wird.
    \item \textbf{Action Age Reward:} Belohnung für Stabilität. Je länger der Agent ohne neue Intervention auskommt, desto höher fällt diese Belohnung aus.
    \item \textbf{Noop Bonus:} Ein fester Belohnungswert für jeden Zeitschritt, in dem keine aktive Steuerungsänderung (Noop) vorgenommen wird.
    \item \textbf{Collision Avoidance Cost:} Negative Belohnung (Strafe) basierend auf der aktuellen Konfliktsituation. Die Strafe steigt mit der Nähe zum Konflikt.
    \item \textbf{Waypoint Bonus:} Ein einmaliger, positiver Belohnungswert beim Erreichen eines Wegpunktes.
    \item \textbf{Collision Penalty:} Eine signifikante Strafe bei Verursachung eines Konflikts (Unterschreitung der Mindestdistanz).
\end{itemize}

Explizit wurde auf eine distanzbasierte Belohnung (\textit{Proximity Reward}) verzichtet, da diese anfällig für \textit{Reward Hacking} ist: Agenten könnten lernen, das Ziel lediglich zu umkreisen, um kontinuierlich Belohnung zu sammeln, ohne es zu erreichen. Die Ausrichtung auf das Ziel (Heading Alignment) löst dieses Navigationsproblem robuster und impliziert zudem eine natürliche Gewichtung der Präzision: Je näher das Flugzeug dem Ziel kommt, desto empfindlicher reagiert der relative Winkel auf Positionsänderungen, was automatisch eine höhere Genauigkeit im Endanflug einfordert.
\vspace{\baselineskip}

Es wurde Wert auf die Verwendung von sogenannten \emph{Dense Rewards} gelegt, also kontinuierlichen Rückmeldungen in jedem Zeitschritt. 
Dies unterstützt den Lernprozess, da der Agent unmittelbares Feedback zur Qualität seiner Zustandsänderungen erhält, im Gegensatz zu \emph{Sparse Rewards}, die nur in unregelmäßigen Abständen vergeben werden.

\subsection{Metriken}
\label{sc:env_metrics}
Die Analyse der Trainings\hyp{} und Evaluationsergebnisse stützt sich auf eine Reihe definierter Metriken. Diese dienen nicht nur der Leistungsbewertung, sondern auch dem Fine\hyp{}Tuning der Belohnungsfunktion.
Die Metriken müssen hinreichend detailliert sein, um zu verifizieren, ob das erlernte Verhalten den Anforderungen der Domäne entspricht.
\vspace{\baselineskip}

Folgende Schlüsselmetriken werden erhoben:
\begin{itemize}
    \item \textbf{isSuccess:} Binärer Indikator für den erfolgreichen Abschluss einer Episode (alle Wegpunkte erreicht, keine Konflikte verursacht).
    \item \textbf{Cumulative Reward:} Die aufsummierte Belohnung über eine gesamte Episode, zusätzlich aufgeschlüsselt nach den einzelnen Reward\hyp{}Komponenten zur detaillierten Analyse.
    \item \textbf{Total Noop Instructions:} Anzahl der Zeitschritte ohne aktive Steuerbefehle.
    \item \textbf{Total Direct Route Instructions:} Häufigkeit, mit der der Befehl \enquote{Direct Route} gewählt wurde.
    \item \textbf{Total Steer Instructions:} Anzahl der aktiven Kursänderungen. Diese Metrik ist ein Indikator für die Arbeitslast des Lotsen und sollte minimiert werden.
    \item \textbf{Average Drift:} Die durchschnittliche Kursabweichung vom Idealweg über die gesamte Episode.
\end{itemize}

\section{Auswahl des RL\hyp{}Algorithmus}
Für das Training des Agenten wurde Proximal Policy Optimization (PPO) gewählt.
PPO gehört zur Klasse der Policy\hyp{}Gradient\hyp{}Methoden (siehe Kapitel \ref{ch:rl_grundlagen}) und hat sich in zahlreichen Anwendungen als stabiler und robuster Standard etabliert \cite{OriginalPPO}.
\vspace{\baselineskip}

Ein zentrales Auswahlkriterium war die praktische Einsetzbarkeit in \textit{Stable Baselines3} (SB3), wo PPO als ausgereifter und weit verbreiteter Algorithmus mit belastbarer Referenzimplementierung vorliegt \cite{sb3,sb3_algos_doc}.
Für die vorliegende Arbeit ist insbesondere die funktionale Breite relevant: Laut SB3\hyp{}Dokumentation unterstützt PPO nativ sowohl kontinuierliche als auch diskrete Aktionsräume (u.\,a. \texttt{Box}, \texttt{Discrete}, \texttt{MultiDiscrete}, \texttt{MultiBinary}) \cite{sb3_algos_doc}.
Damit lässt sich eine einheitliche Trainingspipeline über unterschiedliche Environment\hyp{}Designs hinweg nutzen.
Zusätzlich sind im \textit{sb3\hyp{}contrib}\hyp{}Ökosystem Erweiterungen wie \textit{RecurrentPPO} und \textit{MaskablePPO} verfügbar, was die methodische Anschlussfähigkeit für spätere Experimente erhöht \cite{sb3_algos_doc}.
\vspace{\baselineskip}

Off\hyp{}Policy\hyp{}Verfahren wurden in dieser Arbeit bewusst nicht eingesetzt.
Ein wesentlicher Grund ist, dass in SB3 kein allgemein einsetzbarer Standardalgorithmus verfügbar ist, der sich wahlweise für diskrete und kontinuierliche Aktionen konfigurieren lässt.
Zudem war die von Off\hyp{}Policy\hyp{}Verfahren häufig versprochene Sample\hyp{}Effizienz im vorliegenden Experiment nicht ausschlaggebend, da die Simulationsumgebung eine schnelle Datengenerierung erlaubte.
Off\hyp{}Policy\hyp{}Algorithmen wurden in Vorversuchen dennoch evaluiert. Aufgrund zusätzlichen I/O\hyp{}Overheads lagen ihre Wall\hyp{}Clock\hyp{}Trainingszeiten\footnote{Mit \enquote{Wall\hyp{}Clock\hyp{}Trainingszeit} ist die tatsächlich verstrichene Echtzeit gemeint, also wie lange das Training auf der Uhr dauert.} jedoch über denen von PPO.
Unter diesen Randbedingungen erwies sich PPO als die zweckmäßigere Wahl im Hinblick auf Stabilität, Durchsatz und Entwicklungsaufwand.
\vspace{\baselineskip}

Für die technische Umsetzung wurde die Bibliothek \textit{Stable Baselines3\textunderscore Plus} \cite{sb3_plus} herangezogen. 
Dabei handelt es sich um eine Erweiterung der populären Bibliothek \textit{Stable Baselines3}, die spezifische Anpassungen für hybride (gemischte) Aktionsräume bereitstellt.
Die spezifischen methodischen Anpassungen des PPO\hyp{}Algorithmus an den in Abschnitt \ref{sc:aktionsraum} beschriebenen gemischten Aktionsraum werden im nachfolgenden Kapitel \ref{ch:ppo_bedingt} vertieft.