\chapter{Umgebungs-Konzeption}
\label{ch:konzeption}

Nachdem die theoretischen Fundamente der Flugsicherung und des Reinforcement Learning gelegt sind, widmet sich dieses Kapitel der konkreten Überführung der Problemstellung in ein trainierbares System. Die Konzeption der Lernumgebung ist der entscheidende Entwicklungsschritt, in dem abstrakte Anforderungen in ein mathematisch greifbares Markov-Entscheidungsmodell (MDP) übersetzt werden.
\vspace{\baselineskip}

Die Qualität der hier getroffenen Designentscheidungen – von der Auswahl der Zustandsmerkmale bis zur Ausgestaltung der Belohnungsfunktion – bestimmt maßgeblich, ob der Agent in der Lage sein wird, nicht nur konfliktfreie, sondern auch operativ sinnvolle und effiziente Strategien zu entwickeln. Ein besonderer Schwerpunkt liegt dabei auf der Modellierung des Aktionsraums, da hier der zentrale Hebel zur Reduktion unnötiger Steuerbefehle (Jittering) vermutet wird. Zu diesem Zweck werden zwei konkurrierende Ansätze vorgestellt: ein klassischer diskreter Aktionsraum und ein neuartiger gemischter Ansatz.
\vspace{\baselineskip}

Im Folgenden wird zunächst die technische Integration des Flugsimulators Bluesky sowie die Generierung relevanter Trainingsszenarien erläutert. Darauf aufbauend erfolgt die detaillierte Definition der MDP-Komponenten, bevor abschließend die Wahl des PPO-Algorithmus als geeignetes Lernverfahren begründet wird.

\section{Anforderungen aus der Problemdomäne}
Vor der detaillierten Beschreibung des \ac{RL}\hyp{}Systems ist ein Verständnis der spezifischen Anforderungen der Flugverkehrskontrolle essenziell. 
Der Lotse im angrenzenden Sektor (Adjacent) muss Flugkonflikte frühzeitig erkennen und lösen, wobei gleichzeitig die Effizienz des Verkehrsflusses gewahrt bleiben muss.
Effizienz impliziert in diesem Kontext, dass Flugzeuge möglichst auf direkten Routen geführt werden und unnötige Kurskorrekturen vermieden werden.
Jede Kursänderung erfordert in der Praxis eine Funkkommunikation zwischen Lotse und Pilot, was die kognitive Arbeitsbelastung (Workload) erhöht und die Frequenzbelegung steigert.
Der \ac{RL}\hyp{}Agent soll daher lernen, Konflikte unter Minimierung von Steuerbefehlen zu lösen, um ein realistisches Lotsenverhalten zu emulieren.

\section{Szenarien}
Die Szenarien wurden so gestaltet, dass der Agent mit einer Vielzahl an Verkehrssituationen konfrontiert wird, was essenziell für die Generalisierungsfähigkeit des Modells ist. 
Die Generierung der Szenarien erfolgt stochastisch, folgt jedoch definierten Mustern, um eine breite Abdeckung möglicher Konfliktgeometrien zu gewährleisten. 
Es werden vier grundlegende Szenarientypen im Training unterschieden:
\begin{itemize}
    \item \textbf{Crossing:} Erzeugung von Konflikten durch sich kreuzende Flugrouten.
    \item \textbf{Merging:} Simulation von zusammenlaufendem Verkehr, bei dem Flugzeuge von individuellen Startpunkten auf einen gemeinsamen Zielpunkt oder Wegpunkt zufliegen.
    \item \textbf{Diverging:} Simulation von auseinanderlaufendem Verkehr. Flugzeuge starten gestaffelt auf einer identischen Route, die sich im weiteren Verlauf aufzweigt.
    \item \textbf{Scenariofiles:} Verwendung vordefinierter Szenarien aus externen Dateien, die auf realen Übungen der Fluglotsenausbildung basieren. 
    Diese Szenarien wurden für die Simulation abstrahiert, da das aktuelle Modell ausschließlich laterale Steuerung (Kursänderungen) betrachtet und Höhen\hyp{} oder Geschwindigkeitsanpassungen ausgeklammert werden.
\end{itemize}

Die Auswahl des Szenariotyps während des Trainings erfolgt randomisiert, wobei Crossing\hyp{} und Merging\hyp{}Szenarien aufgrund ihrer höheren Komplexität bei der Konfliktlösung häufiger priorisiert werden.

\section{Simulatorintegration und Environment}
Als Simulationskern dient Bluesky \cite{bluesky}, eine Open\hyp{}Source\hyp{}Flugsimulationsplattform, die eine realistische Modellierung der Flugzeugdynamik bietet und Schnittstellen für externe Steuerungssysteme bereitstellt.
Die Interaktion zwischen der Reinforcement\hyp{}Learning\hyp{}Umgebung (Gym) und Bluesky erfolgt über eine dedizierte Adapter\hyp{}Schicht. Diese übersetzt die Aktionen des Agenten in simulatorverständliche Steuerbefehle und extrahiert im Gegenzug die relevanten Zustandsinformationen für das Training.
\vspace{\baselineskip}

Die modulare Architektur der Adapter\hyp{}Schicht erlaubt einen potenziellen Austausch des Simulators, was im Hinblick auf eine geplante Migration auf den Simulator NewSim der DFS von Bedeutung ist.
Die Entscheidung für Bluesky fiel primär aufgrund der Implementierung in Python. 
Dies ermöglichte eine direkte Integration in das Python\hyp{}basierte \ac{RL}\hyp{}Framework ohne die Notwendigkeit komplexer Schnittstellen für den Datenaustausch zwischen unterschiedlichen Programmiersprachen, was die Implementierungskomplexität signifikant verringerte.
Zudem konnte auf Vorarbeiten des Projekts Bluesky\hyp{}Gym \cite{bluesky-gym} aufgebaut werden.

\section{Technische Optimierung der Lernumgebung}
Ein kritischer, oft unterschätzter Faktor im Reinforcement Learning ist die Ausführungsgeschwindigkeit der Umgebung. Da moderne RL-Algorithmen wie PPO typischerweise Millionen von Interaktionsschritten benötigen, um zu konvergieren, summieren sich selbst minimale Verzögerungen pro Zeitschritt zu erheblichen Wartezeiten. Eine performante Implementierung ist daher nicht nur eine Frage der Effizienz, sondern bestimmt maßgeblich die Entwicklungsgeschwindigkeit, da sie schnellere Iterationszyklen und damit häufigeres Testen von Hypothesen ermöglicht.
\vspace{\baselineskip}

Um diesem Anspruch gerecht zu werden, wurde der Python-Code der Umgebung gezielt auf Performance optimiert. Ein Nadelöhr in Python-basierten Simulationen ist oft der Overhead des Interpreters bei rechenintensiven Schleifen. Zur Beschleunigung kam die Just-in-Time (JIT) Compiler-Bibliothek \textbf{Numba} zum Einsatz. Numba übersetzt Python-Funktionen zur Laufzeit in optimierten Maschinencode, was insbesondere bei mathematischen Berechnungen – wie der Distanzbestimmung zwischen Flugzeugen oder der Koordinatentransformation für den relativen Zustandsraum – zu Geschwindigkeitssteigerungen in der Größenordnung von kompilierten Sprachen führt.
\vspace{\baselineskip}

Zusätzlich wurden Berechnungen wo immer möglich \textbf{vektorisiert}. Statt über Listen von Flugzeugen zu iterieren, werden Operationen mittels der Bibliothek \textit{NumPy} auf ganzen Arrays gleichzeitig ausgeführt. Dies nutzt moderne CPU-Instruktionen (SIMD) effizient aus und minimiert teure Funktionsaufrufe und Kontextwechsel innerhalb der Python-Laufzeitumgebung. Durch diese Kombination aus Kompilierung und Vektorisierung konnte der Durchsatz der Umgebung (Steps per Second) massiv gesteigert werden, was die Experimentierzyklen von Tagen auf Stunden reduzierte.

\section{Visualisierung}
Zur Validierung und Analyse des Agentenverhaltens wurde eine Echtzeit\hyp{}Visualisierung implementiert.
Diese stellt neben der Verkehrslage auch erweiterte Diagnoseinformationen dar, darunter prognostizierte Konfliktpunkte, die aktuellen Entscheidungen des Agenten sowie relevante Merkmale des Zustandsraums.
Die technische Umsetzung erfolgte mittels der Bibliothek Pygame, welche eine effiziente und plattformunabhängige Erstellung von 2D\hyp{}Grafiken innerhalb des Python\hyp{}Ökosystems ermöglicht.

\section{Definition der RL\hyp{}Umgebung}
Die Umgebung (Environment) bildet die Schnittstelle zwischen dem \ac{RL}\hyp{}Agenten und der Simulation. 
Sie definiert den Zustandsraum, den Aktionsraum sowie die Belohnungsfunktion.
Die Implementierung folgt der Spezifikation von OpenAI Gym \cite{OpenAI-Gym}, um die Kompatibilität mit etablierten \ac{RL}\hyp{}Bibliotheken wie Stable Baselines3 \cite{sb3} sicherzustellen.

\subsection{Zustandsraum}
Das Design des Zustandsraums zielt darauf ab, dem Agenten alle relevanten Informationen zur Konflikterkennung und \hyp{}lösung bereitzustellen, ohne die Generalisierungsfähigkeit zu beeinträchtigen. 
Eine hohe Generalisierung ist erforderlich, damit der Agent auch in unbekannten Situationen, die nicht Teil der Trainingsdaten waren, robuste Entscheidungen trifft.
\vspace{\baselineskip}

Absolute Positionsdaten erwiesen sich als ungeeignet, da sie stark an spezifische Szenarien gebunden sind. 
Stattdessen werden relative Positionsdaten verwendet, die eine abstraktere und damit besser generalisierbare Repräsentation der Verkehrslage ermöglichen.
\vspace{\baselineskip}

Ein weiterer technischer Aspekt ist die stetige Repräsentation zyklischer Merkmale. 
Die direkte Verwendung von Winkelgraden ist problematisch, da die Werte $0^\circ$ und $360^\circ$ numerisch weit auseinanderliegen, obwohl sie identische Zustände beschreiben. 
Um Sprungstellen im Zustandsraum zu vermeiden, werden Winkel durch ihre Sinus\hyp{} und Kosinuskomponenten kodiert.
\vspace{\baselineskip}

Da neuronale Netze in der Regel Eingabevektoren fester Länge erwarten, muss die Dimension des Zustandsraums unabhängig von der variablen Anzahl der Flugzeuge im Sektor konstant bleiben. 
Dies wird durch die Festlegung einer maximalen Slot\hyp{}Anzahl für beobachtbare Flugzeuge und das Auffüllen (Padding) ungenutzter Slots mit neutralen Werten erreicht. 
Die Auswahl, welche Flugzeuge in den Zustandsvektor aufgenommen werden, erfolgt priorisiert nach deren Konfliktpotential.
Dieses wird mittels der \ac{CPA}\hyp{}Methode \ref{CPA} bestimmt, wobei die Zeit bis zum nächsten Konflikt sowie die Anzahl der prognostizierten Konflikte die ausschlaggebenden Metriken darstellen.
\vspace{\baselineskip}

Der Zustandsraum beider Umgebungen umfasst folgende Merkmale:
\begin{itemize}
    \item \textbf{Eigener Agent:}
    \begin{itemize}
        \item Aktuelles Heading (Sinus/Kosinus)
        \item Aktuelle Geschwindigkeit
        \item Kursabweichung (Heading\hyp{}Deviation) zum nächsten Wegpunkt
        \item Distanz zum nächsten Wegpunkt
        \item Letzte ausgeführte Aktion (exklusive Noop)
        \item Zeitdifferenz seit der letzten nicht Noop Aktion
        \item Aktuelle Drehrate (Rate of Turn)
        \item Prognostizierte Zeit bis zum Konflikt für verschiedene Kurs-Offsets (z.\,B. $-15^\circ, 0^\circ, +15^\circ$)
        \item Prognostizierte Zeit bis zum Konflikt bei direktem Kurs zum Ziel
    \end{itemize}
    \item \textbf{Konfliktverkehr (sortiert nach Kritikalität):}
    \begin{itemize}
        \item Relativer Positionsvektor
        \item Euklidische Distanz
        \item Relativer Winkel (Sinus/Kosinus)
        \item Geschwindigkeit des Flugzeugs
        \item Annäherungsrate (Closing Rate)
        \item Minimaler Abstand bei konstantem Kurs (\ac{CPA}\hyp{}Distanz)
        \item Zeit bis zum minimalen Abstand (Time to \ac{CPA})
    \end{itemize}
    \item \textbf{Umgebungsverkehr (die 4 nächsten Flugzeuge):}
    \begin{itemize}
        \item Heading (Sinus/Kosinus)
        \item Geschwindigkeit
        \item Kursabweichung zum nächsten Wegpunkt
        \item Distanz zum nächsten Wegpunkt
    \end{itemize}
\end{itemize}

Die Berechnung zukünftiger Konflikte basiert auf dem \ac{CPA}\hyp{}Verfahren (Closest Point of Approach) \cite{CPA}, wobei die Zeit bis zum Konflikt (\emph{Time to CPA}) als Maß für die Kritikalität dient.
\subsection{Aktionsraum}
\label{sc:aktionsraum}
Die entwickelte Lernumgebung beschränkt sich auf die Konfliktlösung mittels Kursänderungen (Lateralsteuerung). 
Diese Eingrenzung dient der Reduktion der Komplexität des Modells. 
Obgleich operative Fluglotsen auch über Geschwindigkeits\hyp{} und Höhenänderungen verfügen, bleibt die Integration dieser Freiheitsgrade zukünftigen Untersuchungen vorbehalten.
\vspace{\baselineskip}

Ein rein kontinuierlicher Aktionsraum erwies sich für die Anforderungen als unzureichend, da er die Modellierung einer expliziten \emph{Noop}-Aktion (No Operation) – also den bewussten Verzicht auf einen Eingriff – erschwert. 
Ein kontinuierlicher Output von $0^\circ$ ist funktional nicht äquivalent zu \emph{Noop}, insbesondere unter Berücksichtigung der Latenz bei der Befehlsumsetzung. 
Wenn im vorangegangenen Zeitschritt eine signifikante Kursänderung eingeleitet wurde, ermöglicht \emph{Noop} dem Agenten, dieses Manöver ohne weitere Störimpulse abzuschließen.
Ein zentrales Lernziel ist die Minimierung der Interventionsrate: Der Agent soll idealerweise nur zwei Aktionen pro Konflikt ausführen – das Ausweichmanöver und die anschließende Rückkehr auf die Route (Direct Route).
\vspace{\baselineskip}

Der Aktionsraum wurde in zwei Varianten implementiert:
\begin{itemize}
    \item \textbf{Diskreter Aktionsraum:} Der Agent wählt aus einem finiten Set an Aktionen, bestehend aus festen Kursänderungen (z.\,B. $-15^\circ, -5^\circ, \dots, +15^\circ$), \emph{Noop} und \emph{Direct Route}.
    \item \textbf{Gemischter Aktionsraum (Hybrid):} Die Entscheidung erfolgt hierarchisch über einen diskreten Aktionstyp und einen zugehörigen kontinuierlichen Parameter:
    \begin{itemize}
        \item \textbf{Steer:} Der Agent bestimmt eine Kursänderung als kontinuierlichen Wert im Intervall $[-180^\circ, +180^\circ]$.
        \item \textbf{Noop:} Der Agent behält den aktuellen Status bei.
        \item \textbf{Direct Route:} Der Agent veranlasst die Rückkehr auf den direkten Kurs zum nächsten Wegpunkt.
    \end{itemize}
\end{itemize}

Die Implementierung des gemischten Aktionsraums stellte besondere Anforderungen an den Lernalgorithmus. 
Kapitel \ref{ch:ppo_bedingt} erläutert die notwendigen Modifikationen am PPO\hyp{}Algorithmus zur Handhabung bedingter Aktionsräume.

\subsection{Belohnungsfunktion}
Das Design der Belohnungsfunktion (Reward Function) erfordert eine sorgfältige Abwägung zwischen den konkurrierenden Zielen der Konfliktvermeidung und der Flugeffizienz.
Um die Komplexität des Lernproblems handhabbar zu halten, wurde die Anzahl der Einflussfaktoren auf ein notwendiges Minimum beschränkt.
Eine übermäßig komplexe Reward\hyp{}Struktur kann die Konvergenz des Modells erschweren, da die Kausalität zwischen Aktion und Belohnung für den Agenten schwerer zu extrahieren ist.
\vspace{\baselineskip}

Die implementierte Belohnungsfunktion setzt sich aus folgenden Komponenten zusammen:
\begin{itemize}
    \item \textbf{Drift Reward:} Belohnung für das Einhalten des direkten Kurses zum Ziel. Die Belohnung skaliert quadratisch mit der Verringerung der Kursabweichung, um präzises Navigieren zu fördern.
    \item \textbf{Action Age Reward:} Belohnung für Stabilität. Je länger der Agent ohne neue Intervention auskommt, desto höher fällt diese Belohnung aus.
    \item \textbf{Noop Bonus:} Ein fester Belohnungswert für jeden Zeitschritt, in dem keine aktive Steuerungsänderung (Noop) vorgenommen wird.
    \item \textbf{Collision Avoidance Cost:} Negative Belohnung (Strafe) basierend auf der aktuellen Konfliktsituation. Die Strafe steigt mit der Nähe zum Konflikt.
    \item \textbf{Waypoint Bonus:} Ein einmaliger, positiver Belohnungswert beim Erreichen eines Wegpunktes.
    \item \textbf{Collision Penalty:} Eine signifikante Strafe bei Verursachung eines Konflikts (Unterschreitung der Mindestdistanz).
\end{itemize}

Es wurde Wert auf die Verwendung von sogenannten \emph{Dense Rewards} gelegt, also kontinuierlichen Rückmeldungen in jedem Zeitschritt. 
Dies unterstützt den Lernprozess, da der Agent unmittelbares Feedback zur Qualität seiner Zustandsänderungen erhält, im Gegensatz zu \emph{Sparse Rewards}, die nur in unregelmäßigen Abständen vergeben werden.

\subsection{Metriken}
\label{sc:env_metrics}
Die Analyse der Trainings\hyp{} und Evaluationsergebnisse stützt sich auf eine Reihe definierter Metriken. Diese dienen nicht nur der Leistungsbewertung, sondern auch dem Fine\hyp{}Tuning der Belohnungsfunktion.
Die Metriken müssen hinreichend detailliert sein, um zu verifizieren, ob das erlernte Verhalten den Anforderungen der Domäne entspricht.
\vspace{\baselineskip}

Folgende Schlüsselmetriken werden erhoben:
\begin{itemize}
    \item \textbf{isSuccess:} Binärer Indikator für den erfolgreichen Abschluss einer Episode (alle Wegpunkte erreicht, keine Konflikte verursacht).
    \item \textbf{Cumulative Reward:} Die aufsummierte Belohnung über eine gesamte Episode, zusätzlich aufgeschlüsselt nach den einzelnen Reward\hyp{}Komponenten zur detaillierten Analyse.
    \item \textbf{Total Noop Instructions:} Anzahl der Zeitschritte ohne aktive Steuerbefehle.
    \item \textbf{Total Direct Route Instructions:} Häufigkeit, mit der der Befehl \glqq Direct Route\grqq{} gewählt wurde.
    \item \textbf{Total Steer Instructions:} Anzahl der aktiven Kursänderungen. Diese Metrik ist ein Indikator für die Arbeitslast des Lotsen und sollte minimiert werden.
    \item \textbf{Average Drift:} Die durchschnittliche Kursabweichung vom Idealweg über die gesamte Episode, als Maß für die Flugeffizienz.
\end{itemize}

\section{Auswahl des RL\hyp{}Algorithmus}
Für das Training des Agenten fiel die Wahl auf den Algorithmus Proximal Policy Optimization (PPO).
PPO zählt zur Klasse der Policy\hyp{}Gradient\hyp{}Methoden (siehe Grundlagenkapitel \ref{ch:rl_grundlagen}) und hat sich aufgrund seiner Stabilität und Robustheit gegenüber Hyperparameter\hyp{}Schwankungen als Standard in vielen Anwendungen etabliert.
\vspace{\baselineskip}

Ein wesentlicher Vorteil von PPO ist die effiziente Parallelisierbarkeit (On\hyp{}Policy), was eine signifikante Beschleunigung des Trainings durch die gleichzeitige Nutzung mehrerer Simulationsinstanzen ermöglicht. 
Im Vergleich zu ebenfalls evaluierten Algorithmen wie Soft Actor\hyp{}Critic (SAC) oder Deep Q\hyp{}Networks (DQN) zeigte PPO in Vorversuchen die beste Balance aus Lernstabilität und Konvergenzgeschwindigkeit für die vorliegende Problemstellung.
Zudem unterstützt PPO nativ sowohl diskrete als auch kontinuierliche Aktionsräume, was eine flexible Anwendung auf die unterschiedlichen Umgebungskonzepte erlaubt.
\vspace{\baselineskip}

Die im Vergleich zu Off\hyp{}Policy\hyp{}Methoden (wie SAC) potenziell geringere Sample\hyp{}Effizienz von PPO wurde durch die hochperformante Simulatoranbindung und die massive Parallelisierung der Trainingsumgebungen kompensiert, sodass sie keinen limitierenden Faktor darstellte.
Ein weiterer entscheidender Faktor für die Wahl von PPO war die Vermeidung von I/O-Flaschenhälsen. 
Während SAC durch das kontinuierliche Management eines Replay Buffers massive Lese- und Schreiboperationen (Memory I/O) verursacht, verarbeitet PPO die gesammelten Erfahrungen linear und ohne diesen Verwaltungs-Overhead.
Dieser Verzicht auf die bandbreitenintensive Verwaltung historischer Daten entlastet den Datendurchsatz signifikant.
Dies ermöglichte auf der verwendeten Entwicklungshardware eine höhere Anzahl paralleler Umgebungen, was die effektive Trainingsgeschwindigkeit im Vergleich zu I/O-intensiveren Algorithmen steigerte.
\vspace{\baselineskip}

Für die technische Umsetzung wurde die Bibliothek \textit{Stable Baselines3\textunderscore Plus} \cite{sb3_plus} herangezogen. 
Dabei handelt es sich um eine Erweiterung der populären Bibliothek \textit{Stable Baselines3}, die spezifische Anpassungen für hybride (gemischte) Aktionsräume bereitstellt.
Die spezifischen methodischen Anpassungen des PPO\hyp{}Algorithmus an den in Abschnitt \ref{sc:aktionsraum} beschriebenen gemischten Aktionsraum werden im nachfolgenden Kapitel \ref{ch:ppo_bedingt} vertieft.