\chapter{Ausblick}
\label{ch:Future_Work}
Während der Entwicklung und Evaluation des \ac{RL}\hyp{}Agenten sind einige Aspekte aufgefallen, die in zukünftigen Arbeiten weiter untersucht werden könnten. Diese Themen besitzen das Potenzial, die Leistung und Anwendbarkeit eines \ac{RL}\hyp{}Agenten im Kontext der Flugverkehrssteuerung weiter zu steigern.

\section*{Multi-Agenten-Systeme}
Die Integration von mehreren \ac{RL}\hyp{}Agenten, die kooperativ zusammenarbeiten, um den Flugverkehr zu steuern, könnte realistischere Szenarien abbilden und die Effizienz der Konfliktlösung erhöhen. Insbesondere in komplexen Luftraumstrukturen ist die Koordination zwischen mehreren Akteuren essenziell.

\section*{Variable Zeitschritte}
Standard-\ac{RL}\hyp{}Algorithmen arbeiten üblicherweise mit festen Zeitschritten. Aktionen ohne expliziten Eingriff (\emph{Noops}) könnten mittels variabler Zeitschritte effizienter modelliert werden, indem die Zeitspanne bis zur nächsten Entscheidung dynamisch variiert wird – je nachdem, wie kritisch die aktuelle Situation bewertet wird. Zu diesem Thema existieren bereits einschlägige Forschungsarbeiten \cite{variable_time_steps}. Eine technische Herausforderung stellt dabei der Discounting-Faktor dar, der bei variablen Zeitintervallen in Standardimplementierungen entsprechend angepasst werden muss.

\section*{Selfplay und Curriculum Learning}
Die Anwendung von Selfplay-Techniken, bei denen der Agent gegen Kopien seiner selbst oder gegen andere trainierte Agenten antritt, könnte die Robustheit und Generalisierungsfähigkeit signifikant verbessern. Ansätze hierzu wurden im Laufe der Entwicklung bereits kurzzeitig evaluiert, scheiterten jedoch am exponentiell gestiegenen Rechenaufwand. 
Ein vielversprechender Ansatz für zukünftige Arbeiten wäre das Training gegen ältere Versionen des eigenen Agenten ("League Training"), um das Verhalten in einer zunehmend realistischen Umgebung zu schärfen. Dieses Konzept des stufenweise anspruchsvolleren Trainings wird auch als \emph{Curriculum Learning} bezeichnet und könnte helfen, lokale Optima zu vermeiden.